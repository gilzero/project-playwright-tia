{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T09:47:51.693045Z",
     "start_time": "2025-01-16T09:47:51.686047Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "942d8693134f030a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T09:49:21.691181Z",
     "start_time": "2025-01-16T09:49:21.687862Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import asyncio\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from playwright.async_api import async_playwright, TimeoutError\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from tqdm.asyncio import tqdm\n",
    "from typing import List, Dict, Optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ba48d67980f01e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T09:49:22.029388Z",
     "start_time": "2025-01-16T09:49:22.026030Z"
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"output/techinasia_ai_news_batch_0_20250116_174236.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1aa78fb570d47fed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T09:50:53.198727Z",
     "start_time": "2025-01-16T09:50:53.192343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    https://www.techinasia.com/news/chinese-unicor...\n",
       "1    https://www.techinasia.com/news/meta-exec-aime...\n",
       "2    https://www.techinasia.com/news/nvidia-build-a...\n",
       "3    https://www.techinasia.com/news/nvidia-backed-...\n",
       "4    https://www.techinasia.com/news/tsmc-q4-profit...\n",
       "Name: article_url, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['article_url'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "574171f180b7e278",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T10:09:44.621083Z",
     "start_time": "2025-01-16T10:09:40.115836Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shanghai-based AI startup MiniMax has released a new family of open-source large language models (LLMs), called MiniMax-01.The launch includes the general-purpose MiniMax-Text-01 model and the multimodal MiniMax-VL-01, which integrates visual capabilities along with text processing.According to benchmark tests shared on MiniMax’s official WeChat account, the foundational language model aligns with global standards in areas such as mathematics, specialized knowledge, instruction-following, and reducing factual errors.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a simple function to fetch the content of the article\n",
    "\n",
    "\n",
    "async def fetch_article_content(url):\n",
    "    async with async_playwright() as playwright:\n",
    "        browser = await playwright.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        try:\n",
    "            await page.goto(url, timeout=10000)\n",
    "            await page.wait_for_selector(\"div#content.content\", timeout=10000)\n",
    "            soup = BeautifulSoup(await page.content(), \"html.parser\")\n",
    "            content = soup.select_one(\"div#content.content\")\n",
    "            return content.get_text(strip=True) if content else None\n",
    "        finally:\n",
    "            await browser.close()\n",
    "\n",
    "\n",
    "url = \"https://www.techinasia.com/news/chinese-unicorn-minimax-unveils-new-ai-models\"\n",
    "content = await fetch_article_content(url)\n",
    "\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee0e12b8635f2333",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T10:21:27.985309Z",
     "start_time": "2025-01-16T10:14:40.136995Z"
    }
   },
   "outputs": [],
   "source": [
    "# For each article URL in the dataframe, fetch the content and store it in a structured JSON format.\n",
    "# Note that a delay will be introduced between each fetch to prevent being blocked by the website.\n",
    "# Additionally, user agents will be randomized and shuffled to avoid detection as a bot.\n",
    "\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1\"\n",
    "]\n",
    "\n",
    "async def fetch_article_content_with_delay(url):\n",
    "    async with async_playwright() as playwright:\n",
    "        browser = await playwright.chromium.launch(headless=True)\n",
    "        page = await browser.new_page(user_agent=random.choice(user_agents))\n",
    "        try:\n",
    "            await page.goto(url, timeout=10000)\n",
    "            await page.wait_for_selector(\"div#content.content\", timeout=10000)\n",
    "            soup = BeautifulSoup(await page.content(), \"html.parser\")\n",
    "            content = soup.select_one(\"div#content.content\")\n",
    "            return content.get_text(strip=True) if content else None\n",
    "        finally:\n",
    "            await browser.close()\n",
    "\n",
    "async def fetch_all_articles(df):\n",
    "    data = []\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        url = row['article_url']\n",
    "        content = await fetch_article_content_with_delay(url)\n",
    "        if content:\n",
    "            data.append({\"article_id\": row['article_id'], \"title\": row['title'], \"url\": url, \"content\": content})\n",
    "        time.sleep(random.uniform(2, 5))  # Introduce delay with randomness\n",
    "    with open(\"articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# run the function\n",
    "# await fetch_all_articles(df)\n",
    "\n",
    "# RuntimeWarning: coroutine 'fetch_all_articles' was never awaited\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "198672ca32ad3e62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T10:51:56.784044Z",
     "start_time": "2025-01-16T10:49:43.510697Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell contains the implementation of functions to fetch article content from a list of URLs in a dataframe.\n",
    "It includes the following key components:\n",
    "1. A list of user agents to randomize requests and avoid detection as a bot.\n",
    "2. An asynchronous function `fetch_article_content_with_delay` that uses Playwright to fetch the content of an article from a given URL with a delay to prevent being blocked.\n",
    "3. An asynchronous function `fetch_all_articles` that iterates over the dataframe, fetches the content for each article URL, and stores the results in a JSON file. It also introduces a random delay between each fetch.\n",
    "4. A commented-out call to `fetch_all_articles` to run the function.\n",
    "5. Adjustable parameters for various ranges and retry attempts.\n",
    "6. Logging configuration to log errors to a file.\n",
    "7. An asynchronous function `fetch_article_content_with_randomization` that fetches article content with additional randomization in user agents and viewport sizes, and simulates scrolling behavior to mimic human interaction.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Adjustable parameters\n",
    "SCROLL_ITERATIONS_RANGE = (2, 4)\n",
    "SCROLL_DISTANCE_RANGE = (50, 200)\n",
    "MOUSE_MOVEMENTS_RANGE = (2, 5)\n",
    "SLEEP_SCROLL_RANGE = (0.3, 0.8)\n",
    "SLEEP_MOUSE_RANGE = (0.1, 0.3)\n",
    "URL_DELAY_RANGE = (0.5, 1)\n",
    "RETRY_ATTEMPTS = 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename=\"errors.log\", level=logging.ERROR,\n",
    "                    format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "async def fetch_article_content_with_randomization(url):\n",
    "    async with async_playwright() as playwright:\n",
    "        browser = await playwright.chromium.launch(headless=True) #  Consider passing a config value\n",
    "        viewport_sizes = [\n",
    "            {\"width\": 1920, \"height\": 1080},\n",
    "            {\"width\": 1366, \"height\": 768},\n",
    "            {\"width\": 1280, \"height\": 720},\n",
    "            {\"width\": 1440, \"height\": 900}\n",
    "        ]\n",
    "        user_agents = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.59\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.59\",\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            page = await browser.new_page(\n",
    "                user_agent=random.choice(user_agents),\n",
    "                viewport=random.choice(viewport_sizes)\n",
    "            )\n",
    "            await page.goto(url, timeout=10000)\n",
    "\n",
    "            await page.wait_for_selector(\"div#content.content\", timeout=10000)\n",
    "\n",
    "\n",
    "            # Simulate scrolling behavior\n",
    "            for _ in range(random.randint(*SCROLL_ITERATIONS_RANGE)):\n",
    "                scroll_position = random.randint(*SCROLL_DISTANCE_RANGE)\n",
    "                await page.evaluate(f\"window.scrollBy(0, {scroll_position})\")\n",
    "                await asyncio.sleep(random.uniform(*SLEEP_SCROLL_RANGE))\n",
    "\n",
    "            # Simulate mouse movements\n",
    "            for _ in range(random.randint(*MOUSE_MOVEMENTS_RANGE)):\n",
    "                x, y = random.randint(0, 1920), random.randint(0, 1080)\n",
    "                await page.mouse.move(x, y, steps=random.randint(5, 10))\n",
    "                await asyncio.sleep(random.uniform(*SLEEP_MOUSE_RANGE))\n",
    "\n",
    "            await asyncio.sleep(random.uniform(0.5, 1.0)) # Wait for dynamic content to load\n",
    "\n",
    "            soup = BeautifulSoup(await page.content(), \"html.parser\")\n",
    "            content_selectors = [ # add fallback selectors if any\n",
    "                \"div#content.content\",\n",
    "            ]\n",
    "\n",
    "            content = None\n",
    "            for selector in content_selectors:\n",
    "                content = soup.select_one(selector)\n",
    "                if content:\n",
    "                    break\n",
    "\n",
    "\n",
    "            return content.get_text(strip=True) if content else None\n",
    "\n",
    "        except TimeoutError as te:\n",
    "            logging.error(f\"Timeout error fetching URL {url}: {te}\")\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching URL {url}: {e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            if browser:\n",
    "                await browser.close()\n",
    "\n",
    "\n",
    "async def fetch_all_articles(df):\n",
    "    data = []\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        url = row[\"article_url\"]\n",
    "\n",
    "        for attempt in range(RETRY_ATTEMPTS):\n",
    "            content = await fetch_article_content_with_randomization(url)\n",
    "            if content:\n",
    "                data.append({\n",
    "                    \"article_id\": row[\"article_id\"],\n",
    "                    \"title\": row[\"title\"],\n",
    "                    \"url\": url,\n",
    "                    \"content\": content\n",
    "                })\n",
    "                break # If successful break out of retry loop\n",
    "            else:\n",
    "                if attempt < RETRY_ATTEMPTS - 1:\n",
    "                    await asyncio.sleep(random.uniform(1, 3) * (attempt + 1)) # Exponential backoff\n",
    "                else:\n",
    "                    logging.error(f\"Failed to fetch URL {url} after {RETRY_ATTEMPTS} attempts.\") # log error after all retries fail\n",
    "        await asyncio.sleep(random.uniform(*URL_DELAY_RANGE))  # Delay between requests\n",
    "\n",
    "    timestamp_str = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"articles_{timestamp_str}.json\"\n",
    "    filepath = os.path.join(\".\", filename)  # To ensure platform compatibility\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "# asyncio.run(fetch_all_articles(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c6095f646576d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f4ddbe8",
   "metadata": {},
   "source": [
    " web scraping system designed to fetch and process content from webpages. \n",
    " \n",
    " It uses Playwright for browser automation, simulating human-like behavior such as scrolling and mouse movements to evade detection. \n",
    " \n",
    " The Config class defines customizable parameters for user interaction simulation, timeouts, retries, and logging. \n",
    " \n",
    " The asynchronous functions manage tasks like content extraction, retry mechanisms for failed requests, and processing a batch of URLs from a DataFrame. \n",
    " \n",
    " Finally, the scraped data is saved to a JSON file with error handling to ensure reliability and robustness. This system is suitable for scalable and dynamic content extraction workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6adabb5af4d102a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T11:04:06.442354Z",
     "start_time": "2025-01-16T10:58:14.968111Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Articles:  22%|██▏       | 11/50 [01:14<04:17,  6.60s/it]/Users/apple/gilzero.dev/project-playwright-tia/.venv/lib/python3.13/site-packages/bs4/builder/__init__.py:295: RuntimeWarning: coroutine 'fetch_all_articles' was never awaited\n",
      "  def _replace_cdata_list_attribute_values(self, tag_name, attrs):\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Fetching Articles: 100%|██████████| 50/50 [05:37<00:00,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to: ./articles_20250116_205828.json\n",
      "Scraping complete. Data saved to JSON.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# This cell contains the configuration settings for the web scraping system.\n",
    "# It defines various parameters to simulate human-like behavior, such as scrolling and mouse movements,\n",
    "# to avoid detection. The configuration includes ranges for scroll iterations, scroll distances,\n",
    "# mouse movements, and sleep durations to mimic human interaction. Additionally, it sets the delay\n",
    "# between URL requests, the number of retry attempts for failed requests, and the timeout for page\n",
    "# navigation and element selection. The viewport sizes and user agents are also specified to emulate\n",
    "# different devices and browsers.\n",
    "\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "class Config:\n",
    "    # --- Scrolling Simulation ---\n",
    "    SCROLL_ITERATIONS_RANGE = (1, 2)\n",
    "    # The range (min, max) for the number of times to simulate scrolling on a page.\n",
    "    # This helps mimic human-like behavior and avoid detection.\n",
    "\n",
    "    SCROLL_DISTANCE_RANGE = (50, 100)\n",
    "    # The range (min, max) for the distance (in pixels) to scroll on each scroll iteration.\n",
    "    # This simulates varying scroll lengths, making the bot less predictable.\n",
    "\n",
    "    # --- Mouse Movement Simulation ---\n",
    "    MOUSE_MOVEMENTS_RANGE = (1, 2)\n",
    "    # The range (min, max) for the number of times to simulate mouse movements on a page.\n",
    "    # This adds another layer of human-like interaction to avoid detection.\n",
    "\n",
    "    # --- Sleep Durations ---\n",
    "    SLEEP_SCROLL_RANGE = (0.1, 0.3)\n",
    "    # The range (min, max) for the duration (in seconds) to sleep after each scroll action.\n",
    "    # This simulates the time a human might take to read and scroll.\n",
    "\n",
    "    SLEEP_MOUSE_RANGE = (0.1, 0.2)\n",
    "    # The range (min, max) for the duration (in seconds) to sleep after each mouse movement.\n",
    "    # This simulates the time a human might take to move the mouse.\n",
    "\n",
    "    # --- Request Delays and Retries ---\n",
    "    URL_DELAY_RANGE = (0.5, 1)\n",
    "    # The range (min, max) for the duration (in seconds) to sleep between fetching different URLs.\n",
    "    # This helps avoid overloading the server and mimics human browsing speed.\n",
    "\n",
    "    RETRY_ATTEMPTS = 3\n",
    "    # The maximum number of times to retry fetching a URL if it fails.\n",
    "    # This helps handle transient network issues and improves reliability.\n",
    "\n",
    "    TIMEOUT = 10000\n",
    "    # The timeout (in milliseconds) for page navigation and element selection.\n",
    "    # This prevents the scraper from getting stuck on slow-loading pages.\n",
    "\n",
    "\n",
    "    VIEWPORT_SIZES = [\n",
    "        {\"width\": 1920, \"height\": 1080},\n",
    "        {\"width\": 1366, \"height\": 768},\n",
    "        {\"width\": 1280, \"height\": 720},\n",
    "        {\"width\": 1440, \"height\": 900},\n",
    "        {\"width\": 1280, \"height\": 1024},\n",
    "        {\"width\": 1024, \"height\": 768},\n",
    "        {\"width\": 800, \"height\": 600},\n",
    "        {\"width\": 1600, \"height\": 900},\n",
    "        {\"width\": 1680, \"height\": 1050},\n",
    "        {\"width\": 1920, \"height\": 1200},\n",
    "        {\"width\": 1600, \"height\": 1200},\n",
    "    ]\n",
    "    USER_AGENTS = [\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.59\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.59\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.59\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.59\",\n",
    "        \"Opera/9.80 (Windows NT 6.1; WOW64) Presto/2.12.388 Version/12.18\",\n",
    "        \"Safari/537.36 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1\",\n",
    "        \"Edge/91.0.864.59 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    ]\n",
    "    CONTENT_SELECTORS = [ # Add fallback selectors if any, refer to file: individual-article-page.html\n",
    "        \"div#content.content\",\n",
    "    ]\n",
    "    LOG_FILE = \"errors.log\"\n",
    "    OUTPUT_DIR = \".\" # Current directory\n",
    "\n",
    "# --- Logging Setup ---\n",
    "logging.basicConfig(filename=Config.LOG_FILE, level=logging.ERROR,\n",
    "                    format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "async def simulate_user_behavior(page, config: Config):\n",
    "    \"\"\"Simulates user scrolling and mouse movements.\"\"\"\n",
    "    for _ in range(random.randint(*config.SCROLL_ITERATIONS_RANGE)):\n",
    "        scroll_position = random.randint(*config.SCROLL_DISTANCE_RANGE)\n",
    "        await page.evaluate(f\"window.scrollBy(0, {scroll_position})\")\n",
    "        await asyncio.sleep(random.uniform(*config.SLEEP_SCROLL_RANGE))\n",
    "\n",
    "    for _ in range(random.randint(*config.MOUSE_MOVEMENTS_RANGE)):\n",
    "        x, y = random.randint(0, 1920), random.randint(0, 1080)\n",
    "        await page.mouse.move(x, y, steps=random.randint(5, 10))\n",
    "        await asyncio.sleep(random.uniform(*config.SLEEP_MOUSE_RANGE))\n",
    "\n",
    "async def extract_content(page, config: Config) -> Optional[str]:\n",
    "    \"\"\"Extracts content from the page using multiple selectors.\"\"\"\n",
    "    await asyncio.sleep(random.uniform(0.5, 1.0))  # Wait for dynamic content\n",
    "    soup = BeautifulSoup(await page.content(), \"html.parser\")\n",
    "    for selector in config.CONTENT_SELECTORS:\n",
    "        content = soup.select_one(selector)\n",
    "        if content:\n",
    "            return content.get_text(strip=True)\n",
    "    return None\n",
    "\n",
    "async def fetch_page_content(url: str, config: Config) -> Optional[str]:\n",
    "    \"\"\"Fetches content from a URL with retries and error handling.\"\"\"\n",
    "    async with async_playwright() as playwright:\n",
    "        browser = await playwright.chromium.launch(headless=True)\n",
    "        try:\n",
    "            page = await browser.new_page(\n",
    "                user_agent=random.choice(config.USER_AGENTS),\n",
    "                viewport=random.choice(config.VIEWPORT_SIZES)\n",
    "            )\n",
    "            await page.goto(url, timeout=config.TIMEOUT)\n",
    "            await page.wait_for_selector(config.CONTENT_SELECTORS[0], timeout=config.TIMEOUT) # Wait for at least one selector to be present\n",
    "            await simulate_user_behavior(page, config)\n",
    "            return await extract_content(page, config)\n",
    "        except TimeoutError as te:\n",
    "            logging.error(f\"Timeout error fetching URL {url}: {te}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching URL {url}: {e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            if browser:\n",
    "                await browser.close()\n",
    "\n",
    "async def fetch_article_with_retries(url: str, config: Config) -> Optional[str]:\n",
    "    \"\"\"Fetches article content with retries.\"\"\"\n",
    "    for attempt in range(config.RETRY_ATTEMPTS):\n",
    "        content = await fetch_page_content(url, config)\n",
    "        if content:\n",
    "            return content\n",
    "        if attempt < config.RETRY_ATTEMPTS - 1:\n",
    "            await asyncio.sleep(random.uniform(1, 3) * (attempt + 1))  # Exponential backoff\n",
    "        else:\n",
    "            logging.error(f\"Failed to fetch URL {url} after {config.RETRY_ATTEMPTS} attempts.\")\n",
    "    return None\n",
    "\n",
    "async def fetch_all_articles(df: pd.DataFrame, config: Config) -> List[Dict]:\n",
    "    \"\"\"Fetches content for all articles in the DataFrame.\"\"\"\n",
    "    data = []\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Fetching Articles\"):\n",
    "        url = row[\"article_url\"]\n",
    "        content = await fetch_article_with_retries(url, config)\n",
    "        if content:\n",
    "            data.append({\n",
    "                \"article_id\": row[\"article_id\"],\n",
    "                \"title\": row[\"title\"],\n",
    "                \"url\": url,\n",
    "                \"content\": content\n",
    "            })\n",
    "        await asyncio.sleep(random.uniform(*config.URL_DELAY_RANGE))\n",
    "    return data\n",
    "\n",
    "def save_to_json(data: List[Dict], config: Config):\n",
    "    \"\"\"Saves the scraped data to a JSON file with error handling.\"\"\"\n",
    "    timestamp_str = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"articles_{timestamp_str}.json\"\n",
    "    filepath = os.path.join(config.OUTPUT_DIR, filename)\n",
    "\n",
    "    try:\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data successfully saved to: {filepath}\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Error: Output directory not found: {config.OUTPUT_DIR}\")\n",
    "        print(f\"Error: Could not save data. Output directory not found: {config.OUTPUT_DIR}\")\n",
    "    except PermissionError:\n",
    "        logging.error(f\"Error: Permission denied to write to: {filepath}\")\n",
    "        print(f\"Error: Could not save data. Permission denied to write to: {filepath}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving data to JSON file: {e}\")\n",
    "        print(f\"Error: Could not save data. An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "# --- Main Program ---\n",
    "config = Config()\n",
    "\n",
    "# load the dataframe\n",
    "file_path = \"output/techinasia_ai_news_batch_0_20250116_174236.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# run the function\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "loop = asyncio.get_event_loop()\n",
    "scraped_data = loop.run_until_complete(fetch_all_articles(df, config))\n",
    "\n",
    "# save the data to a json file\n",
    "save_to_json(scraped_data, config)\n",
    "\n",
    "print(\"Scraping complete. Data saved to JSON.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2d0fa19e783abb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
