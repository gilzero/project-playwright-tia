{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-16T09:47:51.693045Z",
     "start_time": "2025-01-16T09:47:51.686047Z"
    }
   },
   "source": "print(1)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T09:49:21.691181Z",
     "start_time": "2025-01-16T09:49:21.687862Z"
    }
   },
   "cell_type": "code",
   "source": "import pandas as pd\n",
   "id": "942d8693134f030a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T09:49:22.029388Z",
     "start_time": "2025-01-16T09:49:22.026030Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7ba48d67980f01e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T09:49:22.432783Z",
     "start_time": "2025-01-16T09:49:22.429402Z"
    }
   },
   "cell_type": "code",
   "source": "file_path = \"output/techinasia_ai_news_batch_0_20250116_174236.csv\"",
   "id": "ab934f3ba89a88",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T09:49:22.791062Z",
     "start_time": "2025-01-16T09:49:22.771775Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_csv(file_path)",
   "id": "8bcf3f81ffb500dd",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T09:50:53.198727Z",
     "start_time": "2025-01-16T09:50:53.192343Z"
    }
   },
   "cell_type": "code",
   "source": "df['article_url']",
   "id": "1aa78fb570d47fed",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     https://www.techinasia.com/news/chinese-unicor...\n",
       "1     https://www.techinasia.com/news/meta-exec-aime...\n",
       "2     https://www.techinasia.com/news/nvidia-build-a...\n",
       "3     https://www.techinasia.com/news/nvidia-backed-...\n",
       "4     https://www.techinasia.com/news/tsmc-q4-profit...\n",
       "5     https://www.techinasia.com/news/chinese-ai-chi...\n",
       "6     https://www.techinasia.com/news/israel-boost-a...\n",
       "7     https://www.techinasia.com/news/israeli-hospit...\n",
       "8     https://www.techinasia.com/news/google-adds-ai...\n",
       "9     https://www.techinasia.com/news/afp-news-agenc...\n",
       "10    https://www.techinasia.com/news/linkedin-launc...\n",
       "11    https://www.techinasia.com/news/openai-unveils...\n",
       "12    https://www.techinasia.com/news/infrastructure...\n",
       "13    https://www.techinasia.com/news/openai-launche...\n",
       "14    https://www.techinasia.com/news/bytedance-plan...\n",
       "15    https://www.techinasia.com/news/ytl-sea-launch...\n",
       "16    https://www.techinasia.com/news/taiwanese-star...\n",
       "17    https://www.techinasia.com/news/saudi-firm-hal...\n",
       "18    https://www.techinasia.com/news/israeli-cybers...\n",
       "19    https://www.techinasia.com/news/biden-signs-or...\n",
       "20    https://www.techinasia.com/news/aws-general-ca...\n",
       "21    https://www.techinasia.com/news/trump-set-ai-c...\n",
       "22    https://www.techinasia.com/news/openai-urges-a...\n",
       "23    https://www.techinasia.com/news/tsmc-q4-profit...\n",
       "24    https://www.techinasia.com/news/pubg-maker-to-...\n",
       "25    https://www.techinasia.com/news/microsoft-form...\n",
       "26    https://www.techinasia.com/news/tighten-ai-chi...\n",
       "27    https://www.techinasia.com/news/indonesian-dig...\n",
       "28    https://www.techinasia.com/news/googles-automo...\n",
       "29    https://www.techinasia.com/news/adobe-launches...\n",
       "30    https://www.techinasia.com/news/chinese-ai-sta...\n",
       "31    https://www.techinasia.com/news/south-korea-bo...\n",
       "32    https://www.techinasia.com/news/sk-telecom-lau...\n",
       "33    https://www.techinasia.com/news/youtubers-sell...\n",
       "34    https://www.techinasia.com/news/singtel-launch...\n",
       "35    https://www.techinasia.com/news/chinas-deepsee...\n",
       "36    https://www.techinasia.com/news/chinese-firms-...\n",
       "37    https://www.techinasia.com/news/alibaba-cloud-...\n",
       "38    https://www.techinasia.com/news/hyundai-partne...\n",
       "39    https://www.techinasia.com/news/ant-group-conn...\n",
       "40    https://www.techinasia.com/news/openai-meta-ub...\n",
       "41    https://www.techinasia.com/news/openai-conside...\n",
       "42    https://www.techinasia.com/news/us-agencies-pa...\n",
       "43    https://www.techinasia.com/news/microsoft-sues...\n",
       "44    https://www.techinasia.com/news/nvidia-raises-...\n",
       "45    https://www.techinasia.com/news/tsmc-achieves-...\n",
       "46    https://www.techinasia.com/news/musks-attorney...\n",
       "47    https://www.techinasia.com/news/perplexity-int...\n",
       "48    https://www.techinasia.com/news/meta-faces-all...\n",
       "49    https://www.techinasia.com/news/healthcare-sta...\n",
       "Name: article_url, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T10:09:44.621083Z",
     "start_time": "2025-01-16T10:09:40.115836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "async def fetch_article_content(url):\n",
    "    async with async_playwright() as playwright:\n",
    "        browser = await playwright.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        try:\n",
    "            await page.goto(url, timeout=10000)\n",
    "            await page.wait_for_selector(\"div#content.content\", timeout=10000)\n",
    "            soup = BeautifulSoup(await page.content(), \"html.parser\")\n",
    "            content = soup.select_one(\"div#content.content\")\n",
    "            return content.get_text(strip=True) if content else None\n",
    "        finally:\n",
    "            await browser.close()\n",
    "\n",
    "async def main():\n",
    "    url = \"https://www.techinasia.com/news/chinese-unicorn-minimax-unveils-new-ai-models\"\n",
    "    content = await fetch_article_content(url)\n",
    "    print(content)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ],
   "id": "574171f180b7e278",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shanghai-based AI startup MiniMax has released a new family of open-source large language models (LLMs), called MiniMax-01.The launch includes the general-purpose MiniMax-Text-01 model and the multimodal MiniMax-VL-01, which integrates visual capabilities along with text processing.According to benchmark tests shared on MiniMax’s official WeChat account, the foundational language model aligns with global standards in areas such as mathematics, specialized knowledge, instruction-following, and reducing factual errors.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# now for each of the article_url in the dataframe, we can fetch the content. put them in a structured json. please note that we shall introduce a delay between each fetch to avoid being blocked by the website. and introduce randomness and shuffle the user agents to avoid being detected as a bot.\n",
    "\n"
   ],
   "id": "30cae5e4d683af81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T10:21:27.985309Z",
     "start_time": "2025-01-16T10:14:40.136995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1\"\n",
    "]\n",
    "\n",
    "async def fetch_article_content_with_delay(url):\n",
    "    async with async_playwright() as playwright:\n",
    "        browser = await playwright.chromium.launch(headless=True)\n",
    "        page = await browser.new_page(user_agent=random.choice(user_agents))\n",
    "        try:\n",
    "            await page.goto(url, timeout=10000)\n",
    "            await page.wait_for_selector(\"div#content.content\", timeout=10000)\n",
    "            soup = BeautifulSoup(await page.content(), \"html.parser\")\n",
    "            content = soup.select_one(\"div#content.content\")\n",
    "            return content.get_text(strip=True) if content else None\n",
    "        finally:\n",
    "            await browser.close()\n",
    "\n",
    "async def fetch_all_articles(df):\n",
    "    data = []\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        url = row['article_url']\n",
    "        content = await fetch_article_content_with_delay(url)\n",
    "        if content:\n",
    "            data.append({\"article_id\": row['article_id'], \"title\": row['title'], \"url\": url, \"content\": content})\n",
    "        time.sleep(random.uniform(2, 5))  # Introduce delay with randomness\n",
    "    with open(\"articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(fetch_all_articles(df))"
   ],
   "id": "ee0e12b8635f2333",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [06:47<00:00,  8.16s/it]\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import asyncio\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1\"\n",
    "]\n",
    "\n",
    "async def fetch_article_content_with_delay(url):\n",
    "    async with async_playwright() as playwright:\n",
    "        browser = await playwright.chromium.launch(headless=True)\n",
    "        page = await browser.new_page(user_agent=random.choice(user_agents))\n",
    "        try:\n",
    "            await page.goto(url, timeout=10000)\n",
    "            await page.wait_for_selector(\"div#content.content\", timeout=10000)\n",
    "            soup = BeautifulSoup(await page.content(), \"html.parser\")\n",
    "            content = soup.select_one(\"div#content.content\")\n",
    "            return content.get_text(strip=True) if content else None\n",
    "        finally:\n",
    "            await browser.close()\n",
    "\n",
    "async def fetch_all_articles(df):\n",
    "    data = []\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        url = row['article_url']\n",
    "        content = await fetch_article_content_with_delay(url)\n",
    "        if content:\n",
    "            data.append({\"article_id\": row['article_id'], \"title\": row['title'], \"url\": url, \"content\": content})\n",
    "        time.sleep(random.uniform(1, 3))  # Introduce delay with randomness\n",
    "    with open(\"articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(fetch_all_articles(df))"
   ],
   "id": "a6e64eb43095890b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import asyncio\n",
    "import random\n",
    "import time\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "async def fetch_article_content_with_randomization(url):\n",
    "    async with async_playwright() as playwright:\n",
    "        browser = await playwright.chromium.launch(headless=True)\n",
    "        viewport_sizes = [\n",
    "            {\"width\": 1920, \"height\": 1080},\n",
    "            {\"width\": 1366, \"height\": 768},\n",
    "            {\"width\": 1280, \"height\": 720},\n",
    "            {\"width\": 1440, \"height\": 900}\n",
    "        ]\n",
    "        user_agents = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1\"\n",
    "        ]\n",
    "        page = await browser.new_page(\n",
    "            user_agent=random.choice(user_agents),\n",
    "            viewport=random.choice(viewport_sizes)\n",
    "        )\n",
    "        try:\n",
    "            await page.goto(url, timeout=10000)\n",
    "            await page.wait_for_selector(\"div#content.content\", timeout=10000)\n",
    "\n",
    "            # Simulate random scrolling behavior\n",
    "            for _ in range(random.randint(3, 8)):\n",
    "                scroll_position = random.randint(100, 500)\n",
    "                await page.evaluate(f\"window.scrollBy(0, {scroll_position})\")\n",
    "                await asyncio.sleep(random.uniform(0.5, 1.5))\n",
    "\n",
    "            # Simulate random mouse movements\n",
    "            for _ in range(random.randint(5, 10)):\n",
    "                x, y = random.randint(0, 1920), random.randint(0, 1080)\n",
    "                await page.mouse.move(x, y, steps=random.randint(1, 5))\n",
    "                await asyncio.sleep(random.uniform(0.2, 0.8))\n",
    "\n",
    "            soup = BeautifulSoup(await page.content(), \"html.parser\")\n",
    "            content = soup.select_one(\"div#content.content\")\n",
    "            return content.get_text(strip=True) if content else None\n",
    "        finally:\n",
    "            await browser.close()\n",
    "\n",
    "async def fetch_all_articles(df):\n",
    "    data = []\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        url = row[\"article_url\"]\n",
    "        content = await fetch_article_content_with_randomization(url)\n",
    "        if content:\n",
    "            data.append({\n",
    "                \"article_id\": row[\"article_id\"],\n",
    "                \"title\": row[\"title\"],\n",
    "                \"url\": url,\n",
    "                \"content\": content\n",
    "            })\n",
    "        time.sleep(random.uniform(1, 3))  # Introduce delay with randomness\n",
    "    with open(\"articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(fetch_all_articles(df))"
   ],
   "id": "ebdff2194b6b58c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T10:34:06.560947Z",
     "start_time": "2025-01-16T10:31:34.254953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename=\"errors.log\", level=logging.ERROR,\n",
    "                    format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "async def fetch_article_content_with_randomization(url):\n",
    "    async with async_playwright() as playwright:\n",
    "        browser = await playwright.chromium.launch(headless=True)\n",
    "        viewport_sizes = [\n",
    "            {\"width\": 1920, \"height\": 1080},\n",
    "            {\"width\": 1366, \"height\": 768},\n",
    "            {\"width\": 1280, \"height\": 720},\n",
    "            {\"width\": 1440, \"height\": 900}\n",
    "        ]\n",
    "        user_agents = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1\"\n",
    "        ]\n",
    "        try:\n",
    "            page = await browser.new_page(\n",
    "                user_agent=random.choice(user_agents),\n",
    "                viewport=random.choice(viewport_sizes)\n",
    "            )\n",
    "            await page.goto(url, timeout=10000)\n",
    "            await page.wait_for_selector(\"div#content.content\", timeout=10000)\n",
    "\n",
    "            # Simulate random scrolling behavior\n",
    "            for _ in range(random.randint(3, 8)):\n",
    "                scroll_position = random.randint(100, 500)\n",
    "                await page.evaluate(f\"window.scrollBy(0, {scroll_position})\")\n",
    "                await asyncio.sleep(random.uniform(0.5, 1.5))\n",
    "\n",
    "            # Simulate random mouse movements\n",
    "            for _ in range(random.randint(5, 10)):\n",
    "                x, y = random.randint(0, 1920), random.randint(0, 1080)\n",
    "                await page.mouse.move(x, y, steps=random.randint(1, 5))\n",
    "                await asyncio.sleep(random.uniform(0.2, 0.8))\n",
    "\n",
    "            soup = BeautifulSoup(await page.content(), \"html.parser\")\n",
    "            content = soup.select_one(\"div#content.content\")\n",
    "            return content.get_text(strip=True) if content else None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching URL {url}: {e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            await browser.close()\n",
    "\n",
    "async def fetch_all_articles(df):\n",
    "    data = []\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        url = row[\"article_url\"]\n",
    "        content = await fetch_article_content_with_randomization(url)\n",
    "        if content:\n",
    "            data.append({\n",
    "                \"article_id\": row[\"article_id\"],\n",
    "                \"title\": row[\"title\"],\n",
    "                \"url\": url,\n",
    "                \"content\": content\n",
    "            })\n",
    "        time.sleep(random.uniform(1, 3))  # Introduce delay with randomness\n",
    "\n",
    "    timestamp_str = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"articles_{timestamp_str}.json\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(fetch_all_articles(df))"
   ],
   "id": "b130c965820124f8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [02:32<11:33, 16.91s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 71\u001B[0m\n\u001B[1;32m     68\u001B[0m         json\u001B[38;5;241m.\u001B[39mdump(data, f, ensure_ascii\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, indent\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m)\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m---> 71\u001B[0m     \u001B[43masyncio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfetch_all_articles\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/gilzero.dev/project-playwright-tia/venv/lib/python3.13/site-packages/nest_asyncio.py:30\u001B[0m, in \u001B[0;36m_patch_asyncio.<locals>.run\u001B[0;34m(main, debug)\u001B[0m\n\u001B[1;32m     28\u001B[0m task \u001B[38;5;241m=\u001B[39m asyncio\u001B[38;5;241m.\u001B[39mensure_future(main)\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 30\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mloop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_until_complete\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m task\u001B[38;5;241m.\u001B[39mdone():\n",
      "File \u001B[0;32m~/gilzero.dev/project-playwright-tia/venv/lib/python3.13/site-packages/nest_asyncio.py:92\u001B[0m, in \u001B[0;36m_patch_loop.<locals>.run_until_complete\u001B[0;34m(self, future)\u001B[0m\n\u001B[1;32m     90\u001B[0m     f\u001B[38;5;241m.\u001B[39m_log_destroy_pending \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     91\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m f\u001B[38;5;241m.\u001B[39mdone():\n\u001B[0;32m---> 92\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_once\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stopping:\n\u001B[1;32m     94\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/gilzero.dev/project-playwright-tia/venv/lib/python3.13/site-packages/nest_asyncio.py:115\u001B[0m, in \u001B[0;36m_patch_loop.<locals>._run_once\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    108\u001B[0m     heappop(scheduled)\n\u001B[1;32m    110\u001B[0m timeout \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    111\u001B[0m     \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m ready \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stopping\n\u001B[1;32m    112\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mmin\u001B[39m(\u001B[38;5;28mmax\u001B[39m(\n\u001B[1;32m    113\u001B[0m         scheduled[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39m_when \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtime(), \u001B[38;5;241m0\u001B[39m), \u001B[38;5;241m86400\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m scheduled\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m--> 115\u001B[0m event_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_selector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_events(event_list)\n\u001B[1;32m    118\u001B[0m end_time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_clock_resolution\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/selectors.py:548\u001B[0m, in \u001B[0;36mKqueueSelector.select\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    546\u001B[0m ready \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    547\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 548\u001B[0m     kev_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_selector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontrol\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_ev\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    549\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mInterruptedError\u001B[39;00m:\n\u001B[1;32m    550\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ready\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T10:45:35.481639Z",
     "start_time": "2025-01-16T10:39:12.978936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "\n",
    "# Adjustable parameters\n",
    "SCROLL_ITERATIONS_RANGE = (2, 4)\n",
    "SCROLL_DISTANCE_RANGE = (50, 200)\n",
    "MOUSE_MOVEMENTS_RANGE = (2, 5)\n",
    "SLEEP_SCROLL_RANGE = (0.3, 0.8)\n",
    "SLEEP_MOUSE_RANGE = (0.1, 0.3)\n",
    "URL_DELAY_RANGE = (0.5, 1)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename=\"errors.log\", level=logging.ERROR,\n",
    "                    format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "async def fetch_article_content_with_randomization(url):\n",
    "    async with async_playwright() as playwright:\n",
    "        browser = await playwright.chromium.launch(headless=True)\n",
    "        viewport_sizes = [\n",
    "            {\"width\": 1920, \"height\": 1080},\n",
    "            {\"width\": 1366, \"height\": 768},\n",
    "            {\"width\": 1280, \"height\": 720},\n",
    "            {\"width\": 1440, \"height\": 900}\n",
    "        ]\n",
    "        user_agents = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.59\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.59\",\n",
    "        ]\n",
    "        try:\n",
    "            page = await browser.new_page(\n",
    "                user_agent=random.choice(user_agents),\n",
    "                viewport=random.choice(viewport_sizes)\n",
    "            )\n",
    "            await page.goto(url, timeout=10000)\n",
    "            await page.wait_for_selector(\"div#content.content\", timeout=10000)\n",
    "\n",
    "            # Simulate scrolling behavior\n",
    "            for _ in range(random.randint(*SCROLL_ITERATIONS_RANGE)):\n",
    "                scroll_position = random.randint(*SCROLL_DISTANCE_RANGE)\n",
    "                await page.evaluate(f\"window.scrollBy(0, {scroll_position})\")\n",
    "                await asyncio.sleep(random.uniform(*SLEEP_SCROLL_RANGE))\n",
    "\n",
    "            # Simulate mouse movements\n",
    "            for _ in range(random.randint(*MOUSE_MOVEMENTS_RANGE)):\n",
    "                x, y = random.randint(0, 1920), random.randint(0, 1080)\n",
    "                await page.mouse.move(x, y, steps=random.randint(1, 3))\n",
    "                await asyncio.sleep(random.uniform(*SLEEP_MOUSE_RANGE))\n",
    "\n",
    "            soup = BeautifulSoup(await page.content(), \"html.parser\")\n",
    "            content = soup.select_one(\"div#content.content\")\n",
    "            return content.get_text(strip=True) if content else None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching URL {url}: {e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            await browser.close()\n",
    "\n",
    "async def fetch_all_articles(df):\n",
    "    data = []\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        url = row[\"article_url\"]\n",
    "        content = await fetch_article_content_with_randomization(url)\n",
    "        if content:\n",
    "            data.append({\n",
    "                \"article_id\": row[\"article_id\"],\n",
    "                \"title\": row[\"title\"],\n",
    "                \"url\": url,\n",
    "                \"content\": content\n",
    "            })\n",
    "        time.sleep(random.uniform(*URL_DELAY_RANGE))  # Delay between requests\n",
    "\n",
    "    timestamp_str = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"articles_{timestamp_str}.json\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(fetch_all_articles(df))\n"
   ],
   "id": "69c12ca486460d39",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [01:19<05:25,  7.94s/it]/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py:36: RuntimeWarning: coroutine 'fetch_article_content' was never awaited\n",
      "  def __init__(self, callback, args, loop, context=None):\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "100%|██████████| 50/50 [06:22<00:00,  7.65s/it]\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T10:51:56.784044Z",
     "start_time": "2025-01-16T10:49:43.510697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from playwright.async_api import async_playwright, TimeoutError\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Adjustable parameters\n",
    "SCROLL_ITERATIONS_RANGE = (2, 4)\n",
    "SCROLL_DISTANCE_RANGE = (50, 200)\n",
    "MOUSE_MOVEMENTS_RANGE = (2, 5)\n",
    "SLEEP_SCROLL_RANGE = (0.3, 0.8)\n",
    "SLEEP_MOUSE_RANGE = (0.1, 0.3)\n",
    "URL_DELAY_RANGE = (0.5, 1)\n",
    "RETRY_ATTEMPTS = 2\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename=\"errors.log\", level=logging.ERROR,\n",
    "                    format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "async def fetch_article_content_with_randomization(url):\n",
    "    async with async_playwright() as playwright:\n",
    "        browser = await playwright.chromium.launch(headless=True) #  Consider passing a config value\n",
    "        viewport_sizes = [\n",
    "            {\"width\": 1920, \"height\": 1080},\n",
    "            {\"width\": 1366, \"height\": 768},\n",
    "            {\"width\": 1280, \"height\": 720},\n",
    "            {\"width\": 1440, \"height\": 900}\n",
    "        ]\n",
    "        user_agents = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.59\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.59\",\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            page = await browser.new_page(\n",
    "                user_agent=random.choice(user_agents),\n",
    "                viewport=random.choice(viewport_sizes)\n",
    "            )\n",
    "            await page.goto(url, timeout=10000)\n",
    "\n",
    "            await page.wait_for_selector(\"div#content.content\", timeout=10000)\n",
    "\n",
    "\n",
    "            # Simulate scrolling behavior\n",
    "            for _ in range(random.randint(*SCROLL_ITERATIONS_RANGE)):\n",
    "                scroll_position = random.randint(*SCROLL_DISTANCE_RANGE)\n",
    "                await page.evaluate(f\"window.scrollBy(0, {scroll_position})\")\n",
    "                await asyncio.sleep(random.uniform(*SLEEP_SCROLL_RANGE))\n",
    "\n",
    "            # Simulate mouse movements\n",
    "            for _ in range(random.randint(*MOUSE_MOVEMENTS_RANGE)):\n",
    "                x, y = random.randint(0, 1920), random.randint(0, 1080)\n",
    "                await page.mouse.move(x, y, steps=random.randint(5, 10))\n",
    "                await asyncio.sleep(random.uniform(*SLEEP_MOUSE_RANGE))\n",
    "\n",
    "            await asyncio.sleep(random.uniform(0.5, 1.0)) # Wait for dynamic content to load\n",
    "\n",
    "            soup = BeautifulSoup(await page.content(), \"html.parser\")\n",
    "            content_selectors = [ # add fallback selectors if any\n",
    "                \"div#content.content\",\n",
    "            ]\n",
    "\n",
    "            content = None\n",
    "            for selector in content_selectors:\n",
    "                content = soup.select_one(selector)\n",
    "                if content:\n",
    "                    break\n",
    "\n",
    "\n",
    "            return content.get_text(strip=True) if content else None\n",
    "\n",
    "        except TimeoutError as te:\n",
    "            logging.error(f\"Timeout error fetching URL {url}: {te}\")\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching URL {url}: {e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            if browser:\n",
    "                await browser.close()\n",
    "\n",
    "\n",
    "async def fetch_all_articles(df):\n",
    "    data = []\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        url = row[\"article_url\"]\n",
    "\n",
    "        for attempt in range(RETRY_ATTEMPTS):\n",
    "            content = await fetch_article_content_with_randomization(url)\n",
    "            if content:\n",
    "                data.append({\n",
    "                    \"article_id\": row[\"article_id\"],\n",
    "                    \"title\": row[\"title\"],\n",
    "                    \"url\": url,\n",
    "                    \"content\": content\n",
    "                })\n",
    "                break # If successful break out of retry loop\n",
    "            else:\n",
    "                if attempt < RETRY_ATTEMPTS - 1:\n",
    "                    await asyncio.sleep(random.uniform(1, 3) * (attempt + 1)) # Exponential backoff\n",
    "                else:\n",
    "                    logging.error(f\"Failed to fetch URL {url} after {RETRY_ATTEMPTS} attempts.\") # log error after all retries fail\n",
    "        await asyncio.sleep(random.uniform(*URL_DELAY_RANGE))  # Delay between requests\n",
    "\n",
    "    timestamp_str = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"articles_{timestamp_str}.json\"\n",
    "    filepath = os.path.join(\".\", filename)  # To ensure platform compatibility\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "asyncio.run(fetch_all_articles(df))"
   ],
   "id": "198672ca32ad3e62",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [02:13<04:43,  8.32s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 125\u001B[0m\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(filepath, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m    121\u001B[0m         json\u001B[38;5;241m.\u001B[39mdump(data, f, ensure_ascii\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, indent\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m)\n\u001B[0;32m--> 125\u001B[0m \u001B[43masyncio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfetch_all_articles\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/gilzero.dev/project-playwright-tia/venv/lib/python3.13/site-packages/nest_asyncio.py:30\u001B[0m, in \u001B[0;36m_patch_asyncio.<locals>.run\u001B[0;34m(main, debug)\u001B[0m\n\u001B[1;32m     28\u001B[0m task \u001B[38;5;241m=\u001B[39m asyncio\u001B[38;5;241m.\u001B[39mensure_future(main)\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 30\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mloop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_until_complete\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m task\u001B[38;5;241m.\u001B[39mdone():\n",
      "File \u001B[0;32m~/gilzero.dev/project-playwright-tia/venv/lib/python3.13/site-packages/nest_asyncio.py:92\u001B[0m, in \u001B[0;36m_patch_loop.<locals>.run_until_complete\u001B[0;34m(self, future)\u001B[0m\n\u001B[1;32m     90\u001B[0m     f\u001B[38;5;241m.\u001B[39m_log_destroy_pending \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     91\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m f\u001B[38;5;241m.\u001B[39mdone():\n\u001B[0;32m---> 92\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_once\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stopping:\n\u001B[1;32m     94\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/gilzero.dev/project-playwright-tia/venv/lib/python3.13/site-packages/nest_asyncio.py:115\u001B[0m, in \u001B[0;36m_patch_loop.<locals>._run_once\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    108\u001B[0m     heappop(scheduled)\n\u001B[1;32m    110\u001B[0m timeout \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    111\u001B[0m     \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m ready \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stopping\n\u001B[1;32m    112\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mmin\u001B[39m(\u001B[38;5;28mmax\u001B[39m(\n\u001B[1;32m    113\u001B[0m         scheduled[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39m_when \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtime(), \u001B[38;5;241m0\u001B[39m), \u001B[38;5;241m86400\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m scheduled\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m--> 115\u001B[0m event_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_selector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_events(event_list)\n\u001B[1;32m    118\u001B[0m end_time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_clock_resolution\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/selectors.py:548\u001B[0m, in \u001B[0;36mKqueueSelector.select\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    546\u001B[0m ready \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    547\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 548\u001B[0m     kev_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_selector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontrol\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_ev\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    549\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mInterruptedError\u001B[39;00m:\n\u001B[1;32m    550\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ready\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d6c6095f646576d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "g",
   "id": "96ac3fdca7db0fc"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-16T10:58:14.968111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from playwright.async_api import async_playwright, TimeoutError\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.asyncio import tqdm\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# --- Configuration ---\n",
    "class Config:\n",
    "    # --- Scrolling Simulation ---\n",
    "    SCROLL_ITERATIONS_RANGE = (1, 3)\n",
    "    # The range (min, max) for the number of times to simulate scrolling on a page.\n",
    "    # This helps mimic human-like behavior and avoid detection.\n",
    "\n",
    "    SCROLL_DISTANCE_RANGE = (50, 200)\n",
    "    # The range (min, max) for the distance (in pixels) to scroll on each scroll iteration.\n",
    "    # This simulates varying scroll lengths, making the bot less predictable.\n",
    "\n",
    "    # --- Mouse Movement Simulation ---\n",
    "    MOUSE_MOVEMENTS_RANGE = (1, 3)\n",
    "    # The range (min, max) for the number of times to simulate mouse movements on a page.\n",
    "    # This adds another layer of human-like interaction to avoid detection.\n",
    "\n",
    "    # --- Sleep Durations ---\n",
    "    SLEEP_SCROLL_RANGE = (0.2, 0.5)\n",
    "    # The range (min, max) for the duration (in seconds) to sleep after each scroll action.\n",
    "    # This simulates the time a human might take to read and scroll.\n",
    "\n",
    "    SLEEP_MOUSE_RANGE = (0.1, 0.3)\n",
    "    # The range (min, max) for the duration (in seconds) to sleep after each mouse movement.\n",
    "    # This simulates the time a human might take to move the mouse.\n",
    "\n",
    "    # --- Request Delays and Retries ---\n",
    "    URL_DELAY_RANGE = (0.5, 1)\n",
    "    # The range (min, max) for the duration (in seconds) to sleep between fetching different URLs.\n",
    "    # This helps avoid overloading the server and mimics human browsing speed.\n",
    "\n",
    "    RETRY_ATTEMPTS = 3\n",
    "    # The maximum number of times to retry fetching a URL if it fails.\n",
    "    # This helps handle transient network issues and improves reliability.\n",
    "\n",
    "    TIMEOUT = 10000\n",
    "    # The timeout (in milliseconds) for page navigation and element selection.\n",
    "    # This prevents the scraper from getting stuck on slow-loading pages.\n",
    "\n",
    "\n",
    "    VIEWPORT_SIZES = [\n",
    "        {\"width\": 1920, \"height\": 1080},\n",
    "        {\"width\": 1366, \"height\": 768},\n",
    "        {\"width\": 1280, \"height\": 720},\n",
    "        {\"width\": 1440, \"height\": 900}\n",
    "    ]\n",
    "    USER_AGENTS = [\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.59\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.59\",\n",
    "    ]\n",
    "    CONTENT_SELECTORS = [ # Add fallback selectors if any\n",
    "        \"div#content.content\",\n",
    "    ]\n",
    "    LOG_FILE = \"errors.log\"\n",
    "    OUTPUT_DIR = \".\" # Current directory\n",
    "\n",
    "# --- Logging Setup ---\n",
    "logging.basicConfig(filename=Config.LOG_FILE, level=logging.ERROR,\n",
    "                    format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "async def simulate_user_behavior(page, config: Config):\n",
    "    \"\"\"Simulates user scrolling and mouse movements.\"\"\"\n",
    "    for _ in range(random.randint(*config.SCROLL_ITERATIONS_RANGE)):\n",
    "        scroll_position = random.randint(*config.SCROLL_DISTANCE_RANGE)\n",
    "        await page.evaluate(f\"window.scrollBy(0, {scroll_position})\")\n",
    "        await asyncio.sleep(random.uniform(*config.SLEEP_SCROLL_RANGE))\n",
    "\n",
    "    for _ in range(random.randint(*config.MOUSE_MOVEMENTS_RANGE)):\n",
    "        x, y = random.randint(0, 1920), random.randint(0, 1080)\n",
    "        await page.mouse.move(x, y, steps=random.randint(5, 10))\n",
    "        await asyncio.sleep(random.uniform(*config.SLEEP_MOUSE_RANGE))\n",
    "\n",
    "async def extract_content(page, config: Config) -> Optional[str]:\n",
    "    \"\"\"Extracts content from the page using multiple selectors.\"\"\"\n",
    "    await asyncio.sleep(random.uniform(0.5, 1.0))  # Wait for dynamic content\n",
    "    soup = BeautifulSoup(await page.content(), \"html.parser\")\n",
    "    for selector in config.CONTENT_SELECTORS:\n",
    "        content = soup.select_one(selector)\n",
    "        if content:\n",
    "            return content.get_text(strip=True)\n",
    "    return None\n",
    "\n",
    "async def fetch_page_content(url: str, config: Config) -> Optional[str]:\n",
    "    \"\"\"Fetches content from a URL with retries and error handling.\"\"\"\n",
    "    async with async_playwright() as playwright:\n",
    "        browser = await playwright.chromium.launch(headless=True)\n",
    "        try:\n",
    "            page = await browser.new_page(\n",
    "                user_agent=random.choice(config.USER_AGENTS),\n",
    "                viewport=random.choice(config.VIEWPORT_SIZES)\n",
    "            )\n",
    "            await page.goto(url, timeout=config.TIMEOUT)\n",
    "            await page.wait_for_selector(config.CONTENT_SELECTORS[0], timeout=config.TIMEOUT) # Wait for at least one selector to be present\n",
    "            await simulate_user_behavior(page, config)\n",
    "            return await extract_content(page, config)\n",
    "        except TimeoutError as te:\n",
    "            logging.error(f\"Timeout error fetching URL {url}: {te}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching URL {url}: {e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            if browser:\n",
    "                await browser.close()\n",
    "\n",
    "async def fetch_article_with_retries(url: str, config: Config) -> Optional[str]:\n",
    "    \"\"\"Fetches article content with retries.\"\"\"\n",
    "    for attempt in range(config.RETRY_ATTEMPTS):\n",
    "        content = await fetch_page_content(url, config)\n",
    "        if content:\n",
    "            return content\n",
    "        if attempt < config.RETRY_ATTEMPTS - 1:\n",
    "            await asyncio.sleep(random.uniform(1, 3) * (attempt + 1))  # Exponential backoff\n",
    "        else:\n",
    "            logging.error(f\"Failed to fetch URL {url} after {config.RETRY_ATTEMPTS} attempts.\")\n",
    "    return None\n",
    "\n",
    "async def fetch_all_articles(df: pd.DataFrame, config: Config) -> List[Dict]:\n",
    "    \"\"\"Fetches content for all articles in the DataFrame.\"\"\"\n",
    "    data = []\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Fetching Articles\"):\n",
    "        url = row[\"article_url\"]\n",
    "        content = await fetch_article_with_retries(url, config)\n",
    "        if content:\n",
    "            data.append({\n",
    "                \"article_id\": row[\"article_id\"],\n",
    "                \"title\": row[\"title\"],\n",
    "                \"url\": url,\n",
    "                \"content\": content\n",
    "            })\n",
    "        await asyncio.sleep(random.uniform(*config.URL_DELAY_RANGE))\n",
    "    return data\n",
    "\n",
    "def save_to_json(data: List[Dict], config: Config):\n",
    "    \"\"\"Saves the scraped data to a JSON file with error handling.\"\"\"\n",
    "    timestamp_str = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"articles_{timestamp_str}.json\"\n",
    "    filepath = os.path.join(config.OUTPUT_DIR, filename)\n",
    "\n",
    "    try:\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data successfully saved to: {filepath}\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Error: Output directory not found: {config.OUTPUT_DIR}\")\n",
    "        print(f\"Error: Could not save data. Output directory not found: {config.OUTPUT_DIR}\")\n",
    "    except PermissionError:\n",
    "        logging.error(f\"Error: Permission denied to write to: {filepath}\")\n",
    "        print(f\"Error: Could not save data. Permission denied to write to: {filepath}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving data to JSON file: {e}\")\n",
    "        print(f\"Error: Could not save data. An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "# --- Main Program ---\n",
    "config = Config()\n",
    "\n",
    "scraped_data = asyncio.run(fetch_all_articles(df, config))\n",
    "\n",
    "save_to_json(scraped_data, config)\n",
    "\n",
    "print(\"Scraping complete. Data saved to JSON.\")\n",
    "\n"
   ],
   "id": "6adabb5af4d102a3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Articles:  18%|█▊        | 9/50 [01:07<05:07,  7.49s/it]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ff2d0fa19e783abb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
